{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Sharing\n",
    "\n",
    "![](./images/parameters_01.png)\n",
    "\n",
    "The weights, `w`, are shared across patches for a given layer in a CNN to detect the cat above regardless of where in the image it is located.\n",
    "\n",
    "When we are trying to classify a picture of a cat, we don’t care where in the image a cat is. If it’s in the top left or the bottom right, it’s still a cat in our eyes. We would like our CNNs to also possess this ability known as translation invariance. How can we achieve this?\n",
    "\n",
    "As we saw earlier, **the classification of a given patch in an image is determined by the weights and biases corresponding to that patch**.\n",
    "\n",
    "If we want a cat that’s in the top left patch to be classified in the same way as a cat in the bottom right patch, **we need the weights and biases corresponding to those patches to be the same**, so that they are classified the same way.\n",
    "\n",
    "This is exactly what we do in CNNs. The weights and biases we learn for a given output layer are shared across all patches in a given input layer. Note that as we increase the depth of our filter, the number of weights and biases we have to learn still increases, as the weights aren't shared across the output channels.\n",
    "\n",
    "There’s an additional benefit to sharing our parameters. If we did not reuse the same weights across all patches, we would have to learn new parameters for every single patch and hidden layer neuron pair. This does not scale well, especially for higher fidelity images. Thus, sharing parameters not only helps us with translation invariance, but also gives us a smaller, more scalable model.\n",
    "\n",
    "\n",
    "### Padding\n",
    "![](./images/parameters_02_padding.png)\n",
    "\n",
    "Let's say we have a `5x5` grid (as shown above) and a filter of size `3x3` with a stride of `1`. What's the width and height of the next layer?\n",
    "\n",
    "As we can see, the width and height of each subsequent layer decreases in the above scheme.\n",
    "\n",
    "In an ideal world, we'd be able to maintain the same width and height across layers so that we can continue to add layers without worrying about the dimensionality shrinking and so that we have consistency. How might we achieve this? One way is to simply add a border of `0`s to our original `5x5` image. You can see what this looks like in the below image.\n",
    "\n",
    "![](./images/parameters_03_padding.png)\n",
    "The same grid with `0` padding.\n",
    "\n",
    "This would expand our original image to a `7x7`. With this, we now see how our next layer's size is again a `5x5`, keeping our dimensionality consistent.\n",
    "\n",
    "\n",
    "### Dimensionality\n",
    "From what we've learned so far, how can we calculate the number of neurons of each layer in our CNN?\n",
    "\n",
    "Given:\n",
    "* our input layer has a width of `W` and a height of `H`\n",
    "* our convolutional layer has a filter size `F`\n",
    "* we have a stride of `S`\n",
    "* a padding of `P`\n",
    "* and the number of filters `K`,\n",
    "\n",
    "\n",
    "the following formula gives us the width of the next layer: `W_out =[ (W−F+2P)/S] + 1`.\n",
    "\n",
    "The output height would be `H_out = [(H-F+2P)/S] + 1`.\n",
    "\n",
    "And the **output depth would be equal to the number of filters** `D_out = K`.\n",
    "\n",
    "The output volume would be `W_out * H_out * D_out`.\n",
    "\n",
    "Knowing the dimensionality of each additional layer helps us understand how large our model is and how our decisions around filter size and stride affect the size of our network.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz: Convolution Output Shape\n",
    "\n",
    "### Setup\n",
    "H = height, W = width, D = depth\n",
    "\n",
    "* We have an input of shape 32x32x3 (HxWxD)\n",
    "* 20 filters of shape 8x8x3 (HxWxD)\n",
    "* A stride of 2 for both the height and width (S)\n",
    "* With padding of size 1 (P)\n",
    "* Recall the formula for calculating the new height or width:\n",
    "\n",
    "```\n",
    "new_height = (input_height - filter_height + 2 * P)/S + 1\n",
    "new_width = (input_width - filter_width + 2 * P)/S + 1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the shape of the output?\n",
    "\n",
    "The answer format is HxWxD, so if you think the new height is 9, new width is 9, and new depth is 5, then type 9x9x5.\n",
    "\n",
    "The answer is 14x14x20.\n",
    "\n",
    "We can get the new height and width with the formula resulting in:\n",
    "```\n",
    "(32 - 8 + 2 * 1)/2 + 1 = 14\n",
    "(32 - 8 + 2 * 1)/2 + 1 = 14\n",
    "```\n",
    "\n",
    "The new depth is equal to the number of filters, which is 20.\n",
    "\n",
    "This would correspond to the following code:\n",
    "```\n",
    "input = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "filter_weights = tf.Variable(tf.truncated_normal((8, 8, 3, 20))) # (height, width, input_depth, output_depth)\n",
    "filter_bias = tf.Variable(tf.zeros(20))\n",
    "strides = [1, 2, 2, 1] # (batch, height, width, depth)\n",
    "padding = 'SAME'\n",
    "conv = tf.nn.conv2d(input, filter_weights, strides, padding) + filter_bias\n",
    "\n",
    "```\n",
    "\n",
    "Note the output shape of `conv` will be [1, 16, 16, 20]. It's 4D to account for batch size, but more importantly, it's not [1, 14, 14, 20]. This is because the padding algorithm TensorFlow uses is not exactly the same as the one above. An alternative algorithm is to switch `padding` from `'SAME'` to `'VALID'` which would result in an output shape of [1, 13, 13, 20]. If you're curious how padding works in TensorFlow, read [this document](https://www.tensorflow.org/api_docs/python/tf/nn/convolution).\n",
    "\n",
    "In summary TensorFlow uses the following equation for 'SAME' vs 'VALID'\n",
    "\n",
    "SAME Padding, the output height and width are computed as:\n",
    "\n",
    "`out_height` = ceil(float(in_height) / float(strides[1]))\n",
    "\n",
    "`out_width` = ceil(float(in_width) / float(strides[2]))\n",
    "\n",
    "VALID Padding, the output height and width are computed as:\n",
    "\n",
    "`out_height` = ceil(float(in_height - filter_height + 1) / float(strides[1]))\n",
    "\n",
    "`out_width` = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Parameters\n",
    "\n",
    "We're now going to calculate the number of parameters of the convolutional layer. The answer from the last quiz will come into play here!\n",
    "\n",
    "Being able to calculate the number of parameters in a neural network is useful since we want to have control over how much memory a neural network uses.\n",
    "\n",
    "#### Setup\n",
    "H = height, W = width, D = depth\n",
    "\n",
    "* We have an input of shape 32x32x3 (HxWxD)\n",
    "* 20 filters of shape 8x8x3 (HxWxD)\n",
    "* A stride of 2 for both the height and width (S)\n",
    "* Zero padding of size 1 (P)\n",
    "\n",
    "#### Output Layer\n",
    "14x14x20 (HxWxD)\n",
    "\n",
    "#### Hint\n",
    "Without parameter sharing, each neuron in the output layer must connect to each neuron in the filter. In addition, each neuron in the output layer must also connect to a single bias neuron.\n",
    "\n",
    "Convolution Layer Parameters 1\n",
    "How many parameters does the convolutional layer have (without parameter sharing)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
