{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迷你项目：蒙特卡洛方法\n",
    "\n",
    "在此 notebook 中，你将自己编写很多蒙特卡洛 (MC) 算法的实现。\n",
    "\n",
    "虽然我们提供了一些起始代码，但是你可以删掉这些提示并从头编写代码。\n",
    "\n",
    "### 第 0 部分：探索 BlackjackEnv\n",
    "\n",
    "请使用以下代码单元格创建 [Blackjack](https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py) 环境的实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[2018-01-24 22:30:07,814] Making new env: Blackjack-v0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个状态都是包含以下三个元素的 3 元组：\n",
    "- 玩家的当前点数之和 $\\in \\{0, 1, \\ldots, 31\\}$，\n",
    "- 庄家朝上的牌点数之和  $\\in \\{1, \\ldots, 10\\}$，及\n",
    "- 玩家是否有能使用的王牌（`no` $=0$、`yes` $=1$）。\n",
    "\n",
    "智能体可以执行两个潜在动作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    STICK = 0\n",
    "    HIT = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过运行以下代码单元格进行验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
    "Discrete(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行以下代码单元格以按照随机策略玩二十一点。  \n",
    "\n",
    "（*代码当前会玩三次二十一点——你可以随意修改该数字，或者多次运行该单元格。该单元格旨在让你体验当智能体与环境互动时返回的输出结果。*）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(3):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        print(state)\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print('End game! Reward: ', reward)\n",
    "            print('You won :)\\n') if reward > 0 else print('You lost :(\\n')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(8, 5, False)\n",
    "End game! Reward:  -1.0\n",
    "You lost :(\n",
    "\n",
    "(16, 8, False)\n",
    "End game! Reward:  -1\n",
    "You lost :(\n",
    "\n",
    "(13, 10, False)\n",
    "End game! Reward:  -1\n",
    "You lost :(\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第 1 部分：MC 预测 - 状态值\n",
    "\n",
    "在此部分，你将自己编写 MC 预测的实现（用于估算状态值函数）。\n",
    "\n",
    "我们首先将研究以下策略：如果点数之和超过 18，玩家将始终停止出牌。函数`generate_episode_from_limit` 会根据该策略抽取一个阶段。 \n",
    "\n",
    "该函数会接收以下**输入**：\n",
    "- `bj_env`：这是 OpenAI Gym 的 Blackjack 环境的实例。\n",
    "\n",
    "它会返回以下**输出**：\n",
    "- `episode`：这是一个（状态、动作、奖励）元组列表，对应的是 $(S_0, A_0, R_1, \\ldots, S_{T-1}, A_{T-1}, R_{T})$， 其中 $T$ 是最终时间步。具体而言，`episode[i]` 返回 $(S_i, A_i, R_{i+1})$， `episode[i][0]`、`episode[i][1]`和 `episode[i][2]` 分别返回 $S_i$, $A_i$和 $R_{i+1}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_from_limit(bj_env):\n",
    "    episode = []\n",
    "    state = bj_env.reset()\n",
    "    while True:\n",
    "        action = 0 if state[0] > 18 else 1\n",
    "        next_state, reward, done, info = bj_env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行以下代码单元格以按照该策略玩二十一点。 \n",
    "\n",
    "（*代码当前会玩三次二十一点——你可以随意修改该数字，或者多次运行该单元格。该单元格旨在让你熟悉  `generate_episode_from_limit` 函数的输出结果。*）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(generate_episode_from_limit(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[((14, 1, False), 1, -1)]\n",
    "[((14, 10, False), 1, -1)]\n",
    "[((11, 2, False), 1, 0), ((21, 2, False), 0, 1.0)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在你已经准备好自己编写 MC 预测的实现了。你可以选择实现首次经历或所有经历 MC 预测；对于 Blackjack 环境，这两种技巧是对等的。\n",
    "\n",
    "你的算法将有三个参数：\n",
    "- `env`：这是 OpenAI Gym 环境的实例。\n",
    "- `num_episodes`：这是通过智能体-环境互动生成的阶段次数。\n",
    "- `generate_episode`：这是返回互动阶段的函数。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "\n",
    "该算法会返回以下输出结果：\n",
    "- `V`：这是一个字典，其中 `V[s]` 是状态 `s` 的估算值。例如，如果代码返回以下输出结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{(4, 7, False): -0.38775510204081631, (18, 6, False): -0.58434296365330851, (13, 2, False): -0.43409090909090908, (6, 7, False): -0.3783783783783784, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则状态 `(4, 7, False)` 的值估算为  `-0.38775510204081631`。\n",
    "\n",
    "如果你不知道如何在 Python 中使用 `defaultdict`，建议查看[此源代码](https://www.accelebrate.com/blog/using-defaultdict-python/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def mc_prediction_v(env, num_episodes, generate_episode, gamma=1.0):\n",
    "    # initialize empty dictionary of lists\n",
    "    returns = defaultdict(list)\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # generate an episode\n",
    "        episode = generate_episode(env)\n",
    "        # obtain the states, actions, and rewards\n",
    "        states, actions, rewards = zip(*episode)\n",
    "        # prepare for discounting\n",
    "        discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "        # calculate and store the return for each visit in the episode\n",
    "        for i, state in enumerate(states):\n",
    "            returns[state].append(sum(rewards[i:]*discounts[:-(1+i)]))\n",
    "    # calculate the state-value function estimate\n",
    "    V = {k: np.mean(v) for k, v in returns.items()}\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用以下单元格计算并绘制状态值函数估算值。 (_用于绘制值函数的代码来自[此源代码](https://github.com/dennybritz/reinforcement-learning/blob/master/lib/plotting.py)，并且稍作了修改。_）\n",
    "\n",
    "要检查你的实现是否正确，应将以下图与解决方案 notebook **Monte_Carlo_Solution.ipynb** 中的对应图进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import plot_blackjack_values\n",
    "\n",
    "# obtain the value function\n",
    "V = mc_prediction_v(env, 500000, generate_episode_from_limit)\n",
    "\n",
    "# plot the value function\n",
    "plot_blackjack_values(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode 500000/500000.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_14_1.png)\n",
    "\n",
    "\n",
    "### 第 2 部分：MC 预测 - 动作值\n",
    "\n",
    "在此部分，你将自己编写 MC 预测的实现（用于估算动作值函数）。  \n",
    "\n",
    "我们首先将研究以下策略：如果点数之和超过 18，玩家将_几乎_始终停止出牌。具体而言，如果点数之和大于 18，她选择动作 `STICK` 的概率是 80%；如果点数之和不大于 18，她选择动作  `HIT` 的概率是 80%。函数 `generate_episode_from_limit_stochastic` 会根据该策略抽取一个阶段。 \n",
    "\n",
    "该函数会接收以下**输入**：\n",
    "- `bj_env`：这是 OpenAI Gym 的 Blackjack 环境的实例。\n",
    "\n",
    "该算法会返回以下**输出结果**：\n",
    "- `episode`: 这是一个（状态、动作、奖励）元组列表，对应的是 $(S_0, A_0, R_1, \\ldots, S_{T-1}, A_{T-1}, R_{T})$， 其中 $T$ 是最终时间步。具体而言，`episode[i]` 返回 $(S_i, A_i, R_{i+1})$， `episode[i][0]`、`episode[i][1]`和 `episode[i][2]` 分别返回 $S_i$, $A_i$和 $R_{i+1}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_from_limit_stochastic(bj_env):\n",
    "    episode = []\n",
    "    state = bj_env.reset()\n",
    "    while True:\n",
    "        probs = [0.8, 0.2] if state[0] > 18 else [0.2, 0.8]\n",
    "        action = np.random.choice(np.arange(2), p=probs)\n",
    "        next_state, reward, done, info = bj_env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在你已经准备好自己编写 MC 预测的实现了。你可以选择实现首次经历或所有经历 MC 预测；对于 Blackjack 环境，这两种技巧是对等的。\n",
    "\n",
    "你的算法将有三个参数：\n",
    "- `env`: 这是 OpenAI Gym 环境的实例。\n",
    "- `num_episodes`：这是通过智能体-环境互动生成的阶段次数。\n",
    "- `generate_episode`：这是返回互动阶段的函数。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "\n",
    "该算法会返回以下输出结果：\n",
    "\n",
    "- `Q`：这是一个字典（一维数组），其中 `Q[s][a]` 是状态 `s` 和动作 `a` 对应的估算动作值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction_q(env, num_episodes, generate_episode, gamma=1.0):\n",
    "    # initialize empty dictionaries of arrays\n",
    "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # generate an episode\n",
    "        episode = generate_episode(env)\n",
    "        # obtain the states, actions, and rewards\n",
    "        states, actions, rewards = zip(*episode)\n",
    "        # prepare for discounting\n",
    "        discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "        # update the sum of the returns, number of visits, and action-value \n",
    "        # function estimates for each state-action pair in the episode\n",
    "        for i, state in enumerate(states):\n",
    "            returns_sum[state][actions[i]] += sum(rewards[i:]*discounts[:-(1+i)])\n",
    "            N[state][actions[i]] += 1.0\n",
    "            Q[state][actions[i]] = returns_sum[state][actions[i]] / N[state][actions[i]]\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请使用以下单元格获取动作值函数估值 $Q$。我们还绘制了相应的状态值函数。\n",
    "\n",
    "要检查你的实现是否正确，应将以下图与解决方案 notebook **Monte_Carlo_Solution.ipynb** 中的对应图进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the action-value function\n",
    "Q = mc_prediction_q(env, 500000, generate_episode_from_limit_stochastic)\n",
    "\n",
    "# obtain the state-value function\n",
    "V_to_plot = dict((k,(k[0]>18)*(np.dot([0.8, 0.2],v)) + (k[0]<=18)*(np.dot([0.2, 0.8],v))) \\\n",
    "         for k, v in Q.items())\n",
    "\n",
    "# plot the state-value function\n",
    "plot_blackjack_values(V_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode 500000/500000.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_20_1.png)\n",
    "\n",
    "\n",
    "### 第 3 部分：MC 控制 - GLIE\n",
    "\n",
    "在此部分，你将自己编写常量-$\\alpha$ MC 控制的实现。  \n",
    "\n",
    "你的算法将有三个参数：\n",
    "- `env`: 这是 OpenAI Gym 环境的实例。\n",
    "- `num_episodes`：这是通过智能体-环境互动生成的阶段次数。\n",
    "- `generate_episode`：这是返回互动阶段的函数。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "\n",
    "该算法会返回以下输出结果：\n",
    "- `Q`：这是一个字典（一维数组），其中 `Q[s][a]` 是状态 `s` 和动作 `a` 对应的估算动作值。\n",
    "- `policy`：这是一个字典，其中 `policy[s]` 会返回智能体在观察状态 `s` 之后选择的动作。\n",
    "\n",
    "（_你可以随意定义其他函数，以帮助你整理代码。_）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_from_Q(env, Q, epsilon, nA):\n",
    "    \"\"\" generates an episode from following the epsilon-greedy policy \"\"\"\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = np.random.choice(np.arange(nA), p=get_probs(Q[state], epsilon, nA)) \\\n",
    "                                    if state in Q else env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode\n",
    "\n",
    "def get_probs(Q_s, epsilon, nA):\n",
    "    \"\"\" obtains the action probabilities corresponding to epsilon-greedy policy \"\"\"\n",
    "    policy_s = np.ones(nA) * epsilon / nA\n",
    "    best_a = np.argmax(Q_s)\n",
    "    policy_s[best_a] = 1 - epsilon + (epsilon / nA)\n",
    "    return policy_s\n",
    "\n",
    "def update_Q_GLIE(env, episode, Q, N, gamma):\n",
    "    \"\"\" updates the action-value function estimate using the most recent episode \"\"\"\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    # prepare for discounting\n",
    "    discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "    for i, state in enumerate(states):\n",
    "        old_Q = Q[state][actions[i]] \n",
    "        old_N = N[state][actions[i]]\n",
    "        Q[state][actions[i]] = old_Q + (sum(rewards[i:]*discounts[:-(1+i)]) - old_Q)/(old_N+1)\n",
    "        N[state][actions[i]] += 1\n",
    "    return Q, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_GLIE(env, num_episodes, gamma=1.0):\n",
    "    nA = env.action_space.n\n",
    "    # initialize empty dictionaries of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    N = defaultdict(lambda: np.zeros(nA))\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # set the value of epsilon\n",
    "        epsilon = 1.0/((i_episode/8000)+1)\n",
    "        # generate an episode by following epsilon-greedy policy\n",
    "        episode = generate_episode_from_Q(env, Q, epsilon, nA)\n",
    "        # update the action-value function estimate using the episode\n",
    "        Q, N = update_Q_GLIE(env, episode, Q, N, gamma)\n",
    "    # determine the policy corresponding to the final action-value function estimate\n",
    "    policy = dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过以下单元格获取估算的最优策略和动作值函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and action-value function\n",
    "policy_glie, Q_glie = mc_control_GLIE(env, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode 500000/500000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，我们将绘制相应的状态值函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the state-value function\n",
    "V_glie = dict((k,np.max(v)) for k, v in Q_glie.items())\n",
    "\n",
    "# plot the state-value function\n",
    "plot_blackjack_values(V_glie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_27_0.png)\n",
    "\n",
    "\n",
    "最后，我们将可视化估算为最优策略的策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import plot_policy\n",
    "\n",
    "# plot the policy\n",
    "plot_policy(policy_glie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_29_0.png)\n",
    "\n",
    "\n",
    "**真**最优策略 $\\pi_*$ 可以在该[教科书](http://go.udacity.com/rl-textbook)的第 82 页找到（下文也提供了）。请将你的最终估算值与最优策略进行比较——它们能够有多接近？如果你对算法的效果不满意，请花时间调整 $\\epsilon$ 的衰减率和/或使该算法运行更多个阶段，以获得更好的结果。\n",
    "\n",
    "![True Optimal Policy](images/optimal.png)\n",
    "\n",
    "### 第 4 部分：MC 控制 - 常量-$\\alpha$\n",
    "\n",
    "在此部分，你将自己编写常量-$\\alpha$ MC 控制的实现。  \n",
    "\n",
    "你的算法将有三个参数：\n",
    "- `env`: 这是 OpenAI Gym 环境的实例。\n",
    "- `num_episodes`：这是通过智能体-环境互动生成的阶段次数。\n",
    "- `generate_episode`：这是返回互动阶段的函数。\n",
    "- `alpha`：这是更新步骤的步长参数。\n",
    "- `gamma`：这是折扣率。它必须是在 0 到 1（含）之间的值，默认值为：`1`。\n",
    "\n",
    "该算法会返回以下输出结果：\n",
    "- `Q`：这是一个字典（一维数组），其中 `Q[s][a]` 是状态 `s` 和动作 `a` 对应的估算动作值。\n",
    "- `policy`：这是一个字典，其中 `policy[s]` 会返回智能体在观察状态 `s` 之后选择的动作。\n",
    "\n",
    "（_你可以随意定义其他函数，以帮助你整理代码。_）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q_alpha(env, episode, Q, alpha, gamma):\n",
    "    \"\"\" updates the action-value function estimate using the most recent episode \"\"\"\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    # prepare for discounting\n",
    "    discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "    for i, state in enumerate(states):\n",
    "        old_Q = Q[state][actions[i]] \n",
    "        Q[state][actions[i]] = old_Q + alpha*(sum(rewards[i:]*discounts[:-(1+i)]) - old_Q)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_alpha(env, num_episodes, alpha, gamma=1.0):\n",
    "    nA = env.action_space.n\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # set the value of epsilon\n",
    "        epsilon = 1.0/((i_episode/8000)+1)\n",
    "        # generate an episode by following epsilon-greedy policy\n",
    "        episode = generate_episode_from_Q(env, Q, epsilon, nA)\n",
    "        # update the action-value function estimate using the episode\n",
    "        Q = update_Q_alpha(env, episode, Q, alpha, gamma)\n",
    "    # determine the policy corresponding to the final action-value function estimate\n",
    "    policy = dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过以下单元格获得估算的最优策略和动作值函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and action-value function\n",
    "policy_alpha, Q_alpha = mc_control_alpha(env, 500000, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode 500000/500000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，我们将绘制相应的状态值函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the state-value function\n",
    "V_alpha = dict((k,np.max(v)) for k, v in Q_alpha.items())\n",
    "\n",
    "# plot the state-value function\n",
    "plot_blackjack_values(V_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_37_0.png)\n",
    "\n",
    "\n",
    "最后，我们将可视化估算为最优策略的策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the policy\n",
    "plot_policy(policy_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_39_0.png)\n",
    "\n",
    "\n",
    "**真**最优策略 $\\pi_*$ 可以在该[教科书](http://go.udacity.com/rl-textbook)的第 82 页找到（下文也提供了）。请将你的最终估算值与最优策略进行比较——它们能够有多接近？如果你对算法的效果不满意，请花时间调整 $\\epsilon$ 的衰减率和/或使该算法运行更多个阶段，以获得更好的结果。\n",
    "\n",
    "![True Optimal Policy](images/optimal.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
